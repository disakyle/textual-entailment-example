{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BT3103 Tutorial\n",
    "## Overview of the education application\n",
    "* The main task of the application is to help students practice how to write a hypothesis based on the factual evidences presented in a given premise;\n",
    "* A written hypothesis can be in either one of the three following logical relationship to the given premise\n",
    "    * Entailment\n",
    "    * Contradiction\n",
    "    * Neutral\n",
    "* The application needs to be able to automatically predict the right relationship between the hypothesis written by the students and the given premise, and return a probability.\n",
    "\n",
    "Link to the demo education application:\n",
    "* https://nplo4g34yh.execute-api.ap-southeast-1.amazonaws.com/default/textual-entailment-dev\n",
    "\n",
    "Code repository:\n",
    "* https://github.com/evilkyyyle/textual-entailment-example\n",
    "\n",
    "## Prerequisites and Environment Setup\n",
    "* An AWS account\n",
    "* Install AWS Command Line Interface (AWS CLI)\n",
    "    * https://docs.aws.amazon.com/comprehend/latest/dg/setup-awscli.html\n",
    "* Install Docker\n",
    "    * https://www.docker.com/products/docker-desktop\n",
    "* Deep Learning Environment\n",
    "    * Tensorflow, PyTorch, or other learning environment of your choice\n",
    "    * For this tutorial, we will be using PyTorch (with Python 3.7)\n",
    "\n",
    "## The NLP Model\n",
    "In this tutorial, we will use an example NLP model provided by PyTorch. PyTorch examples include a readily-trainable model that is suitable for powering our education application. The model will be trained on the Stanford Natural Language Inference (SNLi) dataset. You can find the example code on:\n",
    "* https://github.com/pytorch/examples/tree/master/snli\n",
    "    \n",
    "and the description of the stanford natural language inference (SNLi) dataset on:\n",
    "* https://nlp.stanford.edu/projects/snli/\n",
    "\n",
    "## Application Development Pipline\n",
    "1. Train the NLP model\n",
    "2. Create the model deployment package (Using Docker)\n",
    "    * If the package is less than 250MB, direct deployment on AWS Lambda\n",
    "    * Otherwise, deploy the model on AWS SageMaker (this tutorial)\n",
    "3. Create the inference (prediction) endpoint\n",
    "4. Design the serveless application on AWS lambda\n",
    "\n",
    "## Step 1. Train the NLP Model\n",
    "Download the example code to your training environment. The training environment could be either your local machine, a remote server, or a cloud service. Deep learning model training will be computational intensive. So we recommend training a remote server or a cloud service. \n",
    "For remote server, NUS students could apply for using high-performance computational resource on Singapore National Supercomputing Centre (NSCC). Register your account using the link:\n",
    "* https://user.nscc.sg\n",
    "\n",
    "For cloud service, we recommend Google Colaboratory which offer free high-performance GPU resources.\n",
    "* https://colab.research.google.com\n",
    "\n",
    "You can explore online to find out more about how to train a model on a remote server or using cloud service, this tutorial will not cover this part in detail. After downloaded the example code along with all required Python packages, you should be able to start model training by simply running the `train.py`.\n",
    "Before doing the training, we will add one line to the `train.py` to save the vocabulary file for later usage. Add the following line right after **Line 28**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(inputs.vocab, args.save_path + 'inputs_vocabs.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training may take more than 24 hours depending on the computational resources.\n",
    "\n",
    "## Step 2. Create the Model Deployment Package (with Docker)\n",
    "The main task in this step is to package all codes necessary into an image that can be directly used to run model inference without any additional setup. All codes necessary are included in the github repository under `project/Sagemaker/container`. This container uses a Python micro web framework called **flask** to manage the model inference. Below is the detailed description of the files included in the container\n",
    "* `./textual_entailment` the folder that contains the code for make inference (prediction)\n",
    "    * `./textual_entailment/model.py` copied directly from the PyTorch SNLi example;\n",
    "    * `./textual_entailment/predictor.py` this is the code that you need to write yourself in order to make inference. This python code should be able to receive an input (the premise and the hypothesis) and output its prediction;\n",
    "    * you can leave the rest three files ** `wsgi.py`, `nginx.conf`, and `serve` ** as it is. They handle the routing logic of the micro web framework\n",
    "    \n",
    "The key inference code in `predictor.py` is the **transformation()** function. Inference code is often not directly provided in many open-sourced deep learning model. To wirte the inference code, you need to examine the code structure carefully to understand how the model input data is transformed and how prediction output is organized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/invocations', methods=['POST'])\n",
    "def transformation():\n",
    "    # Get input JSON data\n",
    "    input_json = flask.request.get_json()\n",
    "    json_input = json.dumps(input_json)\n",
    "    example = data.example.Example.fromJSON(json_input, text_fields)\n",
    "    example_list = [example]\n",
    "    # Tokenize data and predict\n",
    "    if isinstance(text_fields, dict):\n",
    "        fields, field_dict=[],text_fields\n",
    "        for field in field_dict.values():\n",
    "            if isinstance(field, list):\n",
    "                fields.extend(field)\n",
    "            else:\n",
    "                fields.append(field)\n",
    "\n",
    "    predict = data.dataset.Dataset(examples = example_list, fields = fields)\n",
    "    predict_iter = data.iterator.Iterator(dataset = predict, batch_size = 1, device=device)\n",
    "    predict_item = next(iter(predict_iter))\n",
    "    model.eval()\n",
    "    answer = model(predict_item)\n",
    "    xmax = int(torch.max(answer, 1)[1])\n",
    "    \n",
    "    # Transform predicted labels (1, 2, and 3) to easier to understand as (Entailment, Contradiction, and Neutral)\n",
    "    prediction = lambda x: 'Entailment' if x == 1 else ('Contradiction' if\n",
    "                       x == 2 else 'Neutral')\n",
    "    label = prediction(xmax)\n",
    "    percentage = float(torch.exp(answer[0][xmax])/torch.sum(torch.exp(answer)))\n",
    "    percentage = '{:.2%}'.format(percentage)\n",
    "\n",
    "    # Transform predictions to JSON\n",
    "    result = {'label': label, 'probability': percentage}\n",
    "    result = json.dumps(result)\n",
    "    return flask.Response(response=result, status=200, mimetype='application/json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other files in the `project/Sagemaker/container` directory:\n",
    "* `./requirements.txt` this file specifies all the required Python libraries for runing the model inference\n",
    "* `./Dockerfile` this file includes the instructions for Docker to build the deployment image\n",
    "* `./build_and_push.sh` this is the shell script to run Docker and push the built image to AWS\n",
    "\n",
    "Before running the shell script, it is important for you to setup AWS CLi first. Follow the AWS documents and make sure that you set the AWS region to **ap-southeast-1** (you can choose whichever region you want, just make sure that everything you upload to AWS is in the same region).\n",
    "\n",
    "To run the shell script, open terminal (macOS) or command prompt (windows) and change directory to the `container` folder and execute:\n",
    "`./build_and_push.sh textual_entailment_image`\n",
    "After running the script, login to your AWS console and direct to ECR service. You should be able to see the image you just uploaded (the Image is around 2GB). **Remmber to save the Image URI elsewhere for later usage.**\n",
    "\n",
    "<img src='images/AWS_ECR.png'>\n",
    "\n",
    "Notice that there is another folder named `local_test` under the `project/Sagemaker/` directory, this is where you can test all you codes locally before push them to AWS. You can find a more detailed tutorial on how to use Docker to  create the deployment package on the link below:\n",
    "* https://machine-learning-company.nl/deploy-machine-learning-model-rest-api-using-aws/\n",
    "\n",
    "## Step 3. Create the Inference (Prediction) Endpoint\n",
    "The endpoint is an API that enable real-time model inference (prediction). Because many deep learning models are often very large (e.g., several millions or billions of weights in deep neural networks), loading the model weights often takes time (from several seconds to several minutes depending on the size of the model). As such, it would not be a good user experience if the application needs to load the model each time before it makes an prediction. The solution is to create a runtime of the model on AWS SageMaker where the model is pre-loaded and ready for fast and real-time prediction. The endpoint provides the interface for the application to communicate with the running model instance. It received the inputs from the application and return the model predictions. What is better is that this setup also allows scalable parallel predictions.\n",
    "\n",
    "Before creating the endpoint, we need to setup the model in SageMaker. First, we need to upload the model artifacts (i.e., the trained `model.pt` and `inputs_vocabs.pt` files) to AWS S3 bucket. On your AWS web console, create your S3 bucket in the **ap-southeast-1** region and upload the following compressed file to the bucket.\n",
    "* https://drive.google.com/file/d/15leOvRO9KSloz3z_RyFV7UpaNR0aEN77/view?usp=sharing\n",
    "(when uploading your own model, remember to compress all files into one `tar.gz` file)\n",
    "\n",
    "<img src='images/AWS_S3.png'>\n",
    "\n",
    "Similarly, **remeber to save the Object URL elsewhere for later usage.** Our model artifacts are around 31MB, but it is not uncommon that sometimes the model artifacts could also be several GB.\n",
    "\n",
    "Now, we can move onto creating the model on SageMaker. Navigate to Amazon SageMaker on the console and find **Models** then **Create Model**. Give your model a name and assign IAM execution role.\n",
    "\n",
    "<img src='images/AWS_SageMaker_Model.png'>\n",
    "\n",
    "In the container section, put the **Image URI** in **Location of inference code image** and **Object URL** to **Localtion of model artifacts**. Then click **Create model** to finishing creating the model.\n",
    "\n",
    "With the model in place, we can proceed to creating the endpoint. Navigate to **Endpoint configurations** and choose **Create endpoint configuration**.\n",
    "\n",
    "<img src='images/AWS_Endpoint.png'>\n",
    "\n",
    "Enter a name for the configuration and add the model we just created. Since our application usage will not be very intensive, we choose the smallest **Instance type** available **ml.t2.medium**. Remember that model runtime on SageMaker costs money. For **ml.t2.medium**, it is around SGD0.08/hour, which should be easliy covered by our AWS Educate program.\n",
    "\n",
    "At this point, all you need to do is to create the endpoint runtime. Navigate to **Endpoints** and click **Create endpoint**. Then simply select the endpoint configuration we just created. It takes severl minutes for AWS to initiate the endpoint. Once the endpoint status turns to <font color=green>InService</font>, it is ready to be used for model inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Design the Serverless Application on AWS Lambda\n",
    "It is up to you to decide what is the best way to design the interface and learning path to ensure good user experience for learners. In this tutorial, we will focus more on the technical side of deploying this application on AWS. There are two main technical issue to be considered:\n",
    "1. Because our application requires a running model instance on AWS SageMaker, we need to handle the connection between SageMaker and Lambda;\n",
    "2. Because SageMaker runtime is costly, we need to design our application in a way that manage the cost without compromizing user experience.\n",
    "\n",
    "### Connecting Lambda to SageMaker\n",
    "In other words, we need to invoke the SageMaker endpoint in the lambda function. To achieve this, you need to give the Lambda IAM role access to SageMaker policy. Navigate to AWS IAM service and find the IAM role you created for your Lambda service. Then choose **Add inline policy**, the following screen should pop up:\n",
    "\n",
    "<img src='images/AWS_IAM.png'>\n",
    "\n",
    "In the policy editor, set the service to SageMaker. In the **Actions** and **Resources** tabs, you can choose the right SageMaker actions and resources your Lambda role has access to. You can simply allow it to have access to all actions and resources. However, it is best for you to find out which actions and resources you need and assign accordingly.\n",
    "\n",
    "Once the role and policy are updated, you could invoke the SageMaker endpoint in the Lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "runtime_client = boto3.client('runtime.sagemaker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "boto3 is an useful AWS SDK for Python. It allows you to connect to various AWS services using Python code. Check out its documentation at:\n",
    "* https://boto3.amazonaws.com/v1/documentation/api/latest/index.html?id=docs_gateway\n",
    "\n",
    "To invoke the created SageMaker endpoint and run prediction, you can write a Python function as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction(input_json):\n",
    "    response = runtime_client.invoke_endpoint(\n",
    "        EndpointName='textual-entailment-endpoint',\n",
    "        Body=input_json,\n",
    "        ContentType='application/json',\n",
    "        Accept='Accept'\n",
    "    )\n",
    "    result = response['Body'].read()\n",
    "    return result.decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function takes in an json input and send it to the running model instance on SageMaker, then it fetchs the json format prediction output. Notice that the boto3 function used here is the `runtime.sagemaker.invoke_endpoint`. You cannot use this function if your Lambda IAM role is not setup to access SageMaker. Moreover, if you use only this SageMaker-related function in Lambda, when you are setting up the Lambda role for SageMaker, you would only need to give the role access to SageMaker **Read** action and **invoke_endpoint** resource. Now you can embed this `run_prediction` function in your Lambda Python code to make model prediction. \n",
    "\n",
    "### Automatic Initiation and Shutdown of the Endpoint\n",
    "Since SageMaker runtime incurs cost, we should not keep the endpoint always running in the background unless our application is always being used and generating incomes. As such, to manage SageMaker running cost, we need to setup a mechanism for SageMaker to automatically delete (shutdown) an endpoint when it is not being used. The mechanism being used here is to automatically check every 20 minutes for the SageMaker endpoint status. If it is being idle for more than 40 minutes, then shutdown the endpoint. In this case, only the first user to use this application after a shutdown would need to wait for the initiation of the endpoint (several minutes), a minor compromise to overall user experience.\n",
    "\n",
    "SageMaker does not provide information on endpoint utilization, it only reports current endpoint status in terms of whether it is running or not. Hence, we can only infer the endpoint idle time by calculating the time lapsed between current time and the last time the endpoint is being invoked. To achieve this, we need a reliable way to log the last endpoint invocation time. In this case, we use AWS **DynamoDB** service along with Lambda temporary storage (dynamoDB needs to be setup separately, and it is very easy). And you need to give your Lambda role access to the **DynamoDB** Actions and Resources.\n",
    "\n",
    "DynamoDB is a no-SQL database service of AWS. It is free to use if the read/write attempts do not exceed 5 reads or writes per second. Considering that our application could potentially be used by more than 5 users simultaneously, we can avoid this potential overutilization by using it in combination with Lambda temporay storage `/tmp`. The problem with Lambda `/tmp` is that it is not a reliable storage space. Any file stored in `/tmp` may only last for several minutes and could be purged without notice. The following functions works with **DynamoDB** and the `/tmp` storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamodb = boto3.client('dynamodb')\n",
    "\n",
    "# Update the UTC time of Endpoint last invocation \n",
    "def update_last_invocation(new_time):\n",
    "    dynamodb.update_item(\n",
    "        TableName='GlobalItems',\n",
    "        Key={'ProjectName':{'S':'textual-entailment'}},\n",
    "        AttributeUpdates={'lastInvocation':{'Action':'PUT','Value':{'S':new_time}}}\n",
    "        )\n",
    "    return\n",
    "\n",
    "# Get the UTC time when the Endpoint is last invoked\n",
    "def get_last_invocation():\n",
    "    item = dynamodb.get_item(\n",
    "        TableName='GlobalItems',\n",
    "        Key={'ProjectName':{'S':'textual-entailment'}}\n",
    "        )\n",
    "    last_invocation = item['Item']['lastInvocation']['S']\n",
    "    last_invocation = datetime.strptime(last_invocation, '%Y-%m-%d %H:%M:%S.%f')\n",
    "    return last_invocation\n",
    "\n",
    "# Create local log in /tmp directory\n",
    "def create_log():\n",
    "    timestamp = str(datetime.utcnow())\n",
    "    update_last_invocation(timestamp)\n",
    "    log_text = {\n",
    "        'lastInvocation': timestamp,\n",
    "        'lastDynamoUpdate': timestamp\n",
    "    }\n",
    "    with open('/tmp/last_invocation.jsonl','w') as f:\n",
    "        json.dump(log_text, f)\n",
    "        f.write('\\n')\n",
    "        f.close()\n",
    "    return\n",
    "\n",
    "# Update the last invocation in /tmp directory to reduce utilization of DynamoDB\n",
    "def local_last_invocation(new_time, mode):\n",
    "    with open('/tmp/last_invocation.jsonl','r') as f:\n",
    "        invocation_text = json.load(f)\n",
    "        f.close()\n",
    "    if mode == 1:\n",
    "        invocation_text['lastDynamoUpdate'] = new_time\n",
    "        with open('/tmp/last_invocation.jsonl','w') as f:\n",
    "            json.dump(invocation_text, f)\n",
    "            f.write('\\n')\n",
    "            f.close()\n",
    "        return\n",
    "    else:\n",
    "        invocation_text['lastInvocation'] = new_time\n",
    "        last_update = invocation_text['lastDynamoUpdate']\n",
    "        invocation_time = datetime.strptime(new_time, '%Y-%m-%d %H:%M:%S.%f')\n",
    "        update_time = datetime.strptime(last_update, '%Y-%m-%d %H:%M:%S.%f')\n",
    "        time_diff = (invocation_time-update_time).total_seconds\n",
    "        with open('/tmp/last_invocation.jsonl','w') as f:\n",
    "            json.dump(invocation_text, f)\n",
    "            f.write('\\n')\n",
    "            f.close()\n",
    "        return time_diff/60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions:\n",
    "* **update_last_invoation()**: update the value of last endpoint invocation time stored in DynamoDB\n",
    "* **get_last_invoation()**: return the value of last endpoint invocation time stored in DynamoDB\n",
    "* **create_log()**: create a json file in `/tmp` directory with two values, and update the dynamodb value\n",
    "    * Time of endpoint last invocation\n",
    "    * Time of last write to DynamoDB\n",
    "* **local_last_invoation()**: two mode of update the local json file in `/tmp`\n",
    "    * Mode 1: update the value of last dynamodb update time in the local file\n",
    "    * Mode 2: update the value of last endpoint invocation time in the local file and return the minutes elapsed between the current last endpoint invocation time and logged last dynamodb update time\n",
    "\n",
    "With these four functions, we can rewrite the **run_prediction()** function to allow a reliable logging of endpoint invocation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the prediction and update Endpoint last invocation time\n",
    "def run_prediction(input_json):\n",
    "    response = runtime_client.invoke_endpoint(\n",
    "        EndpointName='textual-entailment-endpoint',\n",
    "        Body=input_json,\n",
    "        ContentType='application/json',\n",
    "        Accept='Accept'\n",
    "        )\n",
    "    current_time = str(datetime.utcnow())\n",
    "    if os.path.isfile('/tmp/last_invocation.jsonl'):\n",
    "        try:\n",
    "            minutes_diff = local_last_invocation(current_time, 2)\n",
    "        except:\n",
    "            minutes_diff = 0\n",
    "        if minutes_diff > 5:\n",
    "            update_last_invocation(current_time)\n",
    "            local_last_invocation(current_time, 1)\n",
    "    else:\n",
    "        create_log()\n",
    "    result = response['Body'].read()\n",
    "    return result.decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each time the endpoint is invoked, the new function would check first if the local log file in `/tmp` exist or not. If the file exists, update the last endpoint invocation time to the file and get the minutes elapsed to last dynamodb update (If the minutes lapsed is more than 5 minutes, update the dynamodb last endpoint invocation value). If the file does not exist, create a new text file in `/tmp` and update dynamodb with current time. As such, the dynamodb write and read frequency will never exceed the free tier.\n",
    "\n",
    "With last endpoint invocation time properly logged, we would further use another AWS service **BridgeEvent** to conduct the automatic check for endpoine idle time. Similarly, you need to give your Lambda role access to the **BridgeEvent** Actions and Resources. We define a series of functions to achieve this objective:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('sagemaker')\n",
    "eventWatch = boto3.client('events')\n",
    "\n",
    "# Check the SageMaker Endpoint status\n",
    "def get_endpoint_status():\n",
    "    status = None\n",
    "    try:\n",
    "        endpoint_status = client.describe_endpoint(\n",
    "            EndpointName='textual-entailment-endpoint'\n",
    "            )\n",
    "        status = endpoint_status['EndpointStatus']\n",
    "    except:\n",
    "        status = \"notExist\"\n",
    "    return status\n",
    "\n",
    "# Initiate the SageMaker Endpoint\n",
    "def initiate_endpoint():\n",
    "    client.create_endpoint(\n",
    "        EndpointName='textual-entailment-endpoint',\n",
    "        EndpointConfigName='textual-entailment-endpoint-configuration'\n",
    "        )\n",
    "    return\n",
    "    \n",
    "# Delete the SageMaker Endpoint\n",
    "def terminate_endpoint():\n",
    "    client.delete_endpoint(\n",
    "        EndpointName='textual-entailment-endpoint'\n",
    "        )\n",
    "    return\n",
    "\n",
    "# Check the time elapsed between current UTC time and Endpoint last invocation time\n",
    "def check_time_interval():\n",
    "    current_time = datetime.utcnow()\n",
    "    last_invocation = get_last_invocation()\n",
    "    time_diff = (current_time - last_invocation).total_seconds()\n",
    "    return time_diff/60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions:\n",
    "* **get_endpoint_status()**: return the endpoint status. If no endpoint, return **notExist**\n",
    "* **initiate_endpoint()**: create the endpoint using existing configuration\n",
    "* **terminate_endpoint()**: terminate (delete) the endpoint\n",
    "* **check_time_interval()**: return the minutes elapsed between the last endpoint invocation time stored in dynamodb and current time. \n",
    "\n",
    "Navigate your AWS console to **CloudWatch**. Under **Rules**, choose **Create rule**. Setup the rule as shown below:\n",
    "\n",
    "<img src='images/AWS_Event.png'>\n",
    "\n",
    "The event scheduler will now invoke the Lambda Function every 20 minutes once enabled and pass it the json text **{\"CheckStatus\":\"True\"}**.\n",
    "\n",
    "Now add the following codes to your Lambda function before loading the html page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = get_endpoint_status()\n",
    "\n",
    "# If at the time of scheduled checking, time elapsed is more than 40 minutes, then delete the Endpoint.\n",
    "if \"CheckStatus\" in event and event[\"CheckStatus\"]:\n",
    "    time_interval = check_time_interval()\n",
    "    if time_interval > 40 and status == 'InService':\n",
    "        terminate_endpoint()\n",
    "        # Terminate the CloudWatch Event as well\n",
    "        eventWatch.disable_rule(\n",
    "            Name=\"CheckStatus\"\n",
    "            )\n",
    "    return {\n",
    "    \"statusCode\": 200\n",
    "    }\n",
    "\n",
    "# If no Endpoint instance is running, proceed to create the Endpoint\n",
    "if status == \"notExist\":\n",
    "    initiate_endpoint()\n",
    "    create_log()\n",
    "    # Enable the CloudWatch Event for automatic Endpoint shutdown\n",
    "    eventWatch.enable_rule(\n",
    "        Name=\"CheckStatus\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the Lambda function is invoked not by the users but the scheduled event, it will receive the **\"CheckStatus\"** message and check the minutes elapsed between current time and last endpoint invocation time logged in dynamodb. If minutes elapased is more than 40 minutes, shut down the endpoint and disable the CloudWatch events. After the check, it will also terminate the execution of the Lambda function.\n",
    "\n",
    "If the Lambda function is invoked by the user, it will check whether the endpoint exists or not. If not, it will create the endpoint, create a local json log file in `/tmp`, and enable the CloudWatch events."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
